litmodule:
  training.LitModule:
    cfg:
      tokenizer:
        transformers.AutoTokenizer.from_pretrained():
          pretrained_model_name_or_path: Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct
      network:
        transformers.AutoModelForCausalLM.from_pretrained():
          pretrained_model_name_or_path: Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct
          torch_dtype: bfloat16
          device_map: auto
      lora:
        r: 8
        inference_mode: False
        lora_alpha: 32
        lora_dropout: 0.1
      optimizer:
        torch.optim.AdamW: {lr: 3.0e-4}

datamodule:
  data.DataModule:
    cfg:
      train_size: 0.5
      data_src: ./dataset-test-2.json
      dataset:
        data.ChefBotDataset: {}
      train_dataloader:
        torch.utils.data.DataLoader: {batch_size: 2, shuffle: True}
      val_dataloader:
        torch.utils.data.DataLoader: {batch_size: 2, shuffle: False}

trainer:
  lightning.pytorch.Trainer:
    precision: bf16-mixed
    max_epochs: 5
    logger:
      lightning.pytorch.loggers.WandbLogger: {project: chefbot_llm, save_dir: experiments, name: inital-test}
    callbacks: [lightning.pytorch.callbacks.RichProgressBar: {}]
